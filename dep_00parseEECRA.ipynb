{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fields and their positions (tuple for inclusive start:exclusive end) ---\n",
    "FIELD_POSITIONS_52 = {\n",
    "    'yr': (0, 2),\n",
    "    'mo': (2, 4),\n",
    "    'dy': (4, 6),\n",
    "    'hr': (6, 8),\n",
    "    'IB': (8, 9),\n",
    "    'Lat': (9, 14),\n",
    "    'Lon': (14, 19),\n",
    "    'ID': (19, 24),\n",
    "    'LO': (24, 25),\n",
    "    'ww': (25, 27),\n",
    "    'N': (27, 28),\n",
    "    'Nh': (28, 30),\n",
    "    'h': (30, 32),\n",
    "    'CL': (32, 34),\n",
    "    'CM': (34, 36),\n",
    "    'CH': (36, 38),\n",
    "    'AM': (38, 41),\n",
    "    'AH': (41, 44),\n",
    "    'UM': (44, 45),\n",
    "    'UH': (45, 46),\n",
    "    'IC': (46, 48),\n",
    "    'SA': (48, 52),\n",
    "    'RI': (52, 56),\n",
    "    'SLP': (56, 61),\n",
    "    'WS': (61, 64),\n",
    "    'WD': (64, 67),\n",
    "    'AT': (67, 71),\n",
    "    'DD': (71, 74),\n",
    "    'EL_SST': (74, 78), #only sst used since only parsing ocean\n",
    "    'IW': (78, 79),\n",
    "    'IP_IH': (79, 80)\n",
    "}\n",
    "\n",
    "MISSING_VALUE_FLAGS = {\n",
    "    'ID': ['9'],\n",
    "    'ww': ['-1'],\n",
    "    'Nh': ['-1'],\n",
    "    'h': ['-1'],\n",
    "    'CL': ['-1'],\n",
    "    'CM': ['-1'],\n",
    "    'CH': ['-1'],\n",
    "    'AM': ['900'],\n",
    "    'AH': ['900'],\n",
    "    'UM': ['9'],\n",
    "    'UH': ['9'],\n",
    "    'SLP': ['-1'],\n",
    "    'WS': ['-1'],\n",
    "    'WD': ['-1'],\n",
    "    'AT': ['900'],\n",
    "    'DD': ['900'],\n",
    "    'EL_SST': ['9000'],\n",
    "    'IW': ['9'],\n",
    "    'IP_IH': ['9'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_check(val, key):\n",
    "    \"\"\"Cleans section of sequence and checks whether valid\n",
    "\n",
    "    Args:\n",
    "        val (str): section of sequence corresponding to var\n",
    "        key (str): var name\n",
    "\n",
    "    Returns:\n",
    "        int or NaN: value\n",
    "        boolean: True if valid value\n",
    "    \"\"\"\n",
    "    raw = val.strip().lower()\n",
    "    \n",
    "    if key in MISSING_VALUE_FLAGS and raw in MISSING_VALUE_FLAGS[key]:\n",
    "        return (np.nan, True)  # Mark as missing, but valid\n",
    "\n",
    "    try:\n",
    "        intval = int(raw)\n",
    "    except:\n",
    "        return (np.nan, False)  # Cannot parse\n",
    "\n",
    "    # Check if in range\n",
    "    if key in VALID_RANGES:\n",
    "        min_val, max_val = VALID_RANGES[key]\n",
    "        return (intval, min_val <= intval <= max_val)\n",
    "    \n",
    "    return (intval, True)  # If no range given, accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sequence(seq, fieldPositions):\n",
    "    \"\"\"generates clean dictionary of var names and the value from one sequence\n",
    "\n",
    "    Args:\n",
    "        seq (string): one 80-char line of data\n",
    "        fieldPositions (dictionary): the Fields and their positions, there are \n",
    "        two different dictionaries for 1952-97 and 1997-2008\n",
    "\n",
    "    Returns:\n",
    "        dictionary: var names and cleaned values\n",
    "    \"\"\"\n",
    "    seq = seq.strip()\n",
    "    \n",
    "    if len(seq) != 80:\n",
    "        print(len(seq))\n",
    "        print(f\"Less than 80-char, skipping: {seq}\")\n",
    "        return None\n",
    "\n",
    "    parsed = {}\n",
    "\n",
    "    # Populate parsed with key and value\n",
    "    for key, (start, end) in fieldPositions.items():\n",
    "        parsed[key] = seq[start:end]\n",
    "\n",
    "    # Clean value and repopulate\n",
    "    for key in list(parsed.keys()):\n",
    "        val, valid = clean_and_check(parsed[key], key)\n",
    "        parsed[key] = val\n",
    "        if not valid:\n",
    "            print(f\"Invalid field {key} in sequence: {seq}\")\n",
    "            return None\n",
    "\n",
    "    #Create seperate keys for el_sst/ip_ih and add values\n",
    "    if parsed['LO'] == 2: #should only be parsing ocean\n",
    "        parsed['EL'] = np.nan\n",
    "        parsed['SST'] = parsed['EL_SST']\n",
    "        parsed['IP'] = np.nan\n",
    "        parsed['IH'] = parsed['IP_IH']\n",
    "        \n",
    "    else:\n",
    "        print(f\"Invalid LO in sequence: {seq}\")\n",
    "        return None\n",
    "\n",
    "    # Remove EL_SST raw field\n",
    "    del parsed['EL_SST']\n",
    "    del parsed['IP_IH']\n",
    "\n",
    "    return parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_to_df(path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path (string): path to file\n",
    "\n",
    "    Returns:\n",
    "        df\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    parsed_data = []\n",
    "\n",
    "    for line in lines:\n",
    "        result = parse_sequence(line)\n",
    "        if result is not None:\n",
    "            parsed_data.append(result)\n",
    "\n",
    "    df = pd.DataFrame(parsed_data)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "djf = [\"DEC\", \"JAN\", \"FEB\"] #remember this bleeds into another yr\n",
    "mam = [\"MAR\", \"APR\", \"MAY\"] \n",
    "jja = [\"JUN\", \"JUL\", \"AUG\"]\n",
    "son = [\"SEP\", \"OCT\", \"NOV\"]\n",
    "\n",
    "seasons = [djf, mam, jja, son]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processs a data/EECRA/ship_195112_199712\n",
    "\n",
    "#for fn in os.listdir(\"data/EECRA/ship_195112_199712\"):\n",
    "#    df_MonO = parse_file_to_df(\"data/EECRA/ship_195112_199712/\"+fn)\n",
    "#    df_MonO.to_csv(\"data/EECRA/ocean/df_\"+fn+\".csv\", index=False)\n",
    "    \n",
    "\n",
    "#df2 = pd.concat(map(pd.read_csv, [path + 'df_JUN52O.csv', path + 'df_JUL52O.csv', path + 'df_AUG52O.csv']))\n",
    "\n",
    "files = os.listdir(\"data/EECRA/ship_195112_199712\")\n",
    "    \n",
    "for yr in range(52, 97 + 1):\n",
    "    for season in seasons:\n",
    "        \n",
    "        if season[0] == \"DEC\": \n",
    "            fn1 = season[0] + str(yr - 1) + \"O\"   #djf99 is dec 98, jan 99, and feb 99\n",
    "        else:\n",
    "            fn1 = season[0] + str(yr) + \"O\"\n",
    "            \n",
    "        fn2 = season[1] + str(yr) + \"O\"\n",
    "        fn3 = season[2] + str(yr) + \"O\"\n",
    "        \n",
    "        if fn1 not in files or fn2 not in files or fn3 not in files:\n",
    "            print (\"no\", str(season), \"in yr\", str(yr))\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df1 = parse_file_to_df(\"data/EECRA/ship_195112_199712/\"+fn1)\n",
    "            df2 = parse_file_to_df(\"data/EECRA/ship_195112_199712/\"+fn2)\n",
    "            df3 = parse_file_to_df(\"data/EECRA/ship_195112_199712/\"+fn3)\n",
    "        except Exception as e:\n",
    "            print(fn2, \"season files corrupted?\")\n",
    "            print(e)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            seasondf = pd.concat([df1, df2, df3])\n",
    "            seasondf.to_csv(\"data/EECRA/ocean_seasonal/df_\"+season[0][0]+season[1][0]+season[2][0]+str(yr)+\".csv\", index=False)\n",
    "        except Exception as e:\n",
    "            print(\"failed to save\", str(season), \"in yr\", str(yr))\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_JJA57.csv\n",
      "df_JJA80.csv\n",
      "df_JJA94.csv\n",
      "df_DJF66.csv\n",
      "df_DJF72.csv\n",
      "df_DJF73.csv\n",
      "df_DJF67.csv\n",
      "df_JJA95.csv\n",
      "df_JJA81.csv\n",
      "df_JJA56.csv\n",
      "df_JJA68.csv\n",
      "df_JJA54.csv\n",
      "df_JJA97.csv\n",
      "df_JJA83.csv\n",
      "df_DJF59.csv\n",
      "df_DJF71.csv\n",
      "df_DJF65.csv\n",
      "df_DJF64.csv\n",
      "df_DJF70.csv\n",
      "df_DJF58.csv\n",
      "df_JJA82.csv\n",
      "df_JJA96.csv\n",
      "df_JJA55.csv\n",
      "df_JJA69.csv\n",
      "df_JJA79.csv\n",
      "df_JJA92.csv\n",
      "df_JJA86.csv\n",
      "df_DJF74.csv\n",
      "df_DJF60.csv\n",
      "df_DJF61.csv\n",
      "df_DJF75.csv\n",
      "df_JJA87.csv\n",
      "df_JJA93.csv\n",
      "df_JJA78.csv\n",
      "df_JJA52.csv\n",
      "df_JJA85.csv\n",
      "df_JJA91.csv\n",
      "df_DJF88.csv\n",
      "df_DJF63.csv\n",
      "df_DJF77.csv\n",
      "df_DJF76.csv\n",
      "df_DJF62.csv\n",
      "df_DJF89.csv\n",
      "df_JJA90.csv\n",
      "df_JJA84.csv\n",
      "df_JJA53.csv\n",
      "df_SON71.csv\n",
      "df_MAM64.csv\n",
      "df_MAM70.csv\n",
      "df_SON65.csv\n",
      "df_MAM58.csv\n",
      "df_SON59.csv\n",
      "df_SON58.csv\n",
      "df_MAM59.csv\n",
      "df_SON64.csv\n",
      "df_MAM71.csv\n",
      "df_MAM65.csv\n",
      "df_SON70.csv\n",
      "df_MAM73.csv\n",
      "df_SON66.csv\n",
      "df_SON72.csv\n",
      "df_MAM67.csv\n",
      "df_MAM66.csv\n",
      "df_SON73.csv\n",
      "df_SON67.csv\n",
      "df_MAM72.csv\n",
      "df_MAM76.csv\n",
      "df_SON63.csv\n",
      "df_SON77.csv\n",
      "df_MAM62.csv\n",
      "df_MAM89.csv\n",
      "df_SON88.csv\n",
      "df_SON89.csv\n",
      "df_MAM88.csv\n",
      "df_MAM63.csv\n",
      "df_SON76.csv\n",
      "df_SON62.csv\n",
      "df_MAM77.csv\n",
      "df_SON74.csv\n",
      "df_MAM61.csv\n",
      "df_MAM75.csv\n",
      "df_SON60.csv\n",
      "df_SON61.csv\n",
      "df_MAM74.csv\n",
      "df_MAM60.csv\n",
      "df_SON75.csv\n",
      "df_MAM79.csv\n",
      "df_SON78.csv\n",
      "df_SON93.csv\n",
      "df_MAM86.csv\n",
      "df_MAM92.csv\n",
      "df_SON87.csv\n",
      "df_SON86.csv\n",
      "df_MAM93.csv\n",
      "df_MAM87.csv\n",
      "df_SON92.csv\n",
      "df_SON79.csv\n",
      "df_MAM78.csv\n",
      "df_MAM52.csv\n",
      "df_SON53.csv\n",
      "df_MAM91.csv\n",
      "df_SON84.csv\n",
      "df_SON90.csv\n",
      "df_MAM85.csv\n",
      "df_MAM84.csv\n",
      "df_SON91.csv\n",
      "df_SON85.csv\n",
      "df_MAM90.csv\n",
      "df_SON52.csv\n",
      "df_MAM53.csv\n",
      "df_MAM57.csv\n",
      "df_SON56.csv\n",
      "df_MAM94.csv\n",
      "df_SON81.csv\n",
      "df_SON95.csv\n",
      "df_MAM80.csv\n",
      "df_MAM81.csv\n",
      "df_SON94.csv\n",
      "df_SON80.csv\n",
      "df_MAM95.csv\n",
      "df_SON57.csv\n",
      "df_MAM56.csv\n",
      "df_SON69.csv\n",
      "df_MAM68.csv\n",
      "df_SON55.csv\n",
      "df_MAM54.csv\n",
      "df_SON96.csv\n",
      "df_MAM83.csv\n",
      "df_MAM97.csv\n",
      "df_SON82.csv\n",
      "df_SON83.csv\n",
      "df_MAM96.csv\n",
      "df_MAM82.csv\n",
      "df_SON97.csv\n",
      "df_MAM55.csv\n",
      "df_SON54.csv\n",
      "df_MAM69.csv\n",
      "df_SON68.csv\n",
      "df_JJA62.csv\n",
      "df_JJA76.csv\n",
      "df_JJA89.csv\n",
      "df_DJF84.csv\n",
      "df_DJF90.csv\n",
      "df_DJF53.csv\n",
      "df_DJF52.csv\n",
      "df_DJF91.csv\n",
      "df_DJF85.csv\n",
      "df_JJA88.csv\n",
      "df_JJA77.csv\n",
      "df_JJA63.csv\n",
      "df_JJA75.csv\n",
      "df_JJA61.csv\n",
      "df_DJF93.csv\n",
      "df_DJF87.csv\n",
      "df_DJF78.csv\n",
      "df_DJF79.csv\n",
      "df_DJF86.csv\n",
      "df_DJF92.csv\n",
      "df_JJA60.csv\n",
      "df_JJA74.csv\n",
      "df_JJA70.csv\n",
      "df_JJA64.csv\n",
      "df_JJA58.csv\n",
      "df_DJF96.csv\n",
      "df_DJF82.csv\n",
      "df_DJF55.csv\n",
      "df_DJF69.csv\n",
      "df_DJF68.csv\n",
      "df_DJF54.csv\n",
      "df_DJF83.csv\n",
      "df_DJF97.csv\n",
      "df_JJA59.csv\n",
      "df_JJA65.csv\n",
      "df_JJA71.csv\n",
      "df_JJA67.csv\n",
      "df_JJA73.csv\n",
      "df_DJF81.csv\n",
      "df_DJF95.csv\n",
      "df_DJF56.csv\n",
      "df_DJF57.csv\n",
      "df_DJF94.csv\n",
      "df_DJF80.csv\n",
      "df_JJA72.csv\n",
      "df_JJA66.csv\n"
     ]
    }
   ],
   "source": [
    "#made oopsies, need to divide each lat/lon by 100\n",
    "path = \"data/EECRA/ocean_seasonal/\"\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    print(file)\n",
    "    try:\n",
    "        df = pd.read_csv(path+file)\n",
    "        df['Lat'] = df['Lat'] / 100\n",
    "        df['Lon'] = df['Lon'] / 100\n",
    "        df.to_csv(path+file, index=False)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"failed:\"+file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
